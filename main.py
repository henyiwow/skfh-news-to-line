import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import email.utils
from datetime import datetime, timedelta, timezone
import re
import os

# å¾ç’°å¢ƒè®Šæ•¸è®€å–è¨­å®šï¼ˆæ¨è–¦ç”¨æ–¼ GitHub Actionsï¼‰
ACCESS_TOKEN = os.getenv('ACCESS_TOKEN', 'ä½ çš„ LINE Channel Access Token')
USER_ID = os.getenv('USER_ID', 'ä½ çš„ LINE User ID')

# å°ç£æ™‚å€
TW_TZ = timezone(timedelta(hours=8))

# é—œéµå­—å®šç¾©
KEYWORDS = {
    "æ–°å…‰é‡‘æ§": ["æ–°å…‰é‡‘æ§", "æ–°å…‰äººå£½", "æ–°å£½", "å³æ±é€²"],
    "å°æ–°é‡‘æ§": ["å°æ–°é‡‘æ§", "å°æ–°äººå£½", "å°æ–°å£½", "å³æ±äº®"],
    "ä¿éšªç›¸é—œ": ["ä¿éšª", "å£½éšª", "å¥åº·éšª", "æ„å¤–éšª", "äººå£½ä¿éšª", "ç”¢éšª"]
}

# æ’é™¤é—œéµå­—
EXCLUDED_KEYWORDS = ['ä¿éšªå¥—', 'é¿å­•å¥—', 'å¤ªé™½äººå£½', 'å¤§è¥¿éƒ¨äººå£½']

def classify_news(title, content):
    """æ–°èåˆ†é¡å‡½æ•¸"""
    text = (title + " " + content).lower()
    
    for category, keywords in KEYWORDS.items():
        if any(keyword.lower() in text for keyword in keywords):
            return category
    return None

def get_article_summary(url, max_chars=100):
    """ç²å–æ–‡ç« æ‘˜è¦"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç§»é™¤ä¸éœ€è¦çš„æ¨™ç±¤
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
            tag.decompose()
        
        # å°‹æ‰¾æ–‡ç« å…§å®¹çš„å¸¸è¦‹æ¨™ç±¤
        content_selectors = [
            'article p', '.content p', '.article-content p', 
            '.news-content p', '.post-content p', 'main p',
            '.entry-content p', '.story-content p', '.article-body p'
        ]
        
        content_text = ""
        for selector in content_selectors:
            paragraphs = soup.select(selector)
            if paragraphs and len(paragraphs) > 0:
                # å–å‰3æ®µï¼Œéæ¿¾æ‰å¤ªçŸ­çš„æ®µè½
                valid_paragraphs = [p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 20]
                if valid_paragraphs:
                    content_text = " ".join(valid_paragraphs[:2])
                    break
        
        # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šé¸æ“‡å™¨ï¼Œå˜—è©¦æ‰€æœ‰ p æ¨™ç±¤
        if not content_text:
            paragraphs = soup.find_all('p')
            valid_paragraphs = [p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 20]
            if valid_paragraphs:
                content_text = " ".join(valid_paragraphs[:2])
        
        # æ¸…ç†æ–‡æœ¬
        content_text = re.sub(r'\s+', ' ', content_text)
        content_text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€Œã€ã€ã€ï¼ˆï¼‰]', '', content_text)
        content_text = content_text.strip()
        
        # æˆªå–æŒ‡å®šå­—æ•¸
        if len(content_text) > max_chars:
            content_text = content_text[:max_chars] + "..."
        
        return content_text if content_text else "ç„¡æ³•ç²å–æ‘˜è¦"
        
    except Exception as e:
        print(f"ç²å–æ‘˜è¦å¤±æ•— ({url}): {e}")
        return "ç„¡æ³•ç²å–æ‘˜è¦"

def is_taiwan_news(source_name, link):
    """åˆ¤æ–·æ˜¯å¦ç‚ºå°ç£æ–°è"""
    taiwan_sources = [
        'å·¥å•†æ™‚å ±', 'ä¸­åœ‹æ™‚å ±', 'ç¶“æ¿Ÿæ—¥å ±', 'ä¸‰ç«‹æ–°èç¶²', 'è‡ªç”±æ™‚å ±', 'è¯åˆæ–°èç¶²',
        'é¡é€±åˆŠ', 'å°ç£é›…è™', 'é‰…äº¨ç¶²', 'ä¸­æ™‚æ–°èç¶²', 'Ettodayæ–°èé›²',
        'å¤©ä¸‹é›œèªŒ', 'å¥‡æ‘©æ–°è', 'ç¾ä»£ä¿éšª', 'é è¦‹é›œèªŒ', 'è²¡è¨Š', 'å•†æ¥­å‘¨åˆŠ'
    ]
    
    if any(taiwan_source in source_name for taiwan_source in taiwan_sources):
        return True
    if '.tw' in link:
        return True
    return False

def fetch_news():
    """å¾ RSS ç²å–æ–°è"""
    rss_urls = [
        "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+æ–°å£½+OR+å³æ±é€²&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å°æ–°å£½+OR+å³æ±äº®&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=å£½éšª+OR+ä¿éšª+OR+äººå£½ä¿éšª&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    ]
    
    news_by_category = {category: [] for category in KEYWORDS.keys()}
    processed_titles = set()
    
    for rss_url in rss_urls:
        try:
            print(f"æ­£åœ¨æŠ“å–: {rss_url}")
            response = requests.get(rss_url, timeout=15)
            
            if response.status_code != 200:
                print(f"âŒ RSS è«‹æ±‚å¤±æ•—: {response.status_code}")
                continue
                
            root = ET.fromstring(response.content)
            items = root.findall(".//item")
            print(f"æ‰¾åˆ° {len(items)} å‰‡æ–°è")
            
            for item in items:
                try:
                    title_elem = item.find('title')
                    link_elem = item.find('link')
                    pubDate_elem = item.find('pubDate')
                    source_elem = item.find('source')
                    
                    if not all([title_elem, link_elem, pubDate_elem]):
                        continue
                    
                    title = title_elem.text.strip()
                    link = link_elem.text.strip()
                    pubDate_str = pubDate_elem.text.strip()
                    source_name = source_elem.text.strip() if source_elem is not None else "æœªçŸ¥ä¾†æº"
                    
                    # è·³éç„¡æ•ˆæ¨™é¡Œ
                    if not title or title.startswith("Google") or len(title) < 10:
                        continue
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚ºé‡è¤‡æ–°èï¼ˆç°¡åŒ–æ¨™é¡Œæ¯”å°ï¼‰
                    title_normalized = re.sub(r'[^\w\u4e00-\u9fff]', '', title.lower())
                    if title_normalized in processed_titles:
                        continue
                    
                    # æª¢æŸ¥æ’é™¤é—œéµå­—
                    if any(excluded in title for excluded in EXCLUDED_KEYWORDS):
                        continue
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚ºå°ç£æ–°è
                    if not is_taiwan_news(source_name, link):
                        continue
                    
                    # æª¢æŸ¥ç™¼å¸ƒæ™‚é–“ï¼ˆ24å°æ™‚å…§ï¼‰
                    try:
                        pub_datetime = email.utils.parsedate_to_datetime(pubDate_str).astimezone(TW_TZ)
                        now = datetime.now(TW_TZ)
                        if now - pub_datetime > timedelta(hours=24):
                            continue
                    except:
                        # å¦‚æœæ™‚é–“è§£æå¤±æ•—ï¼Œè·³éæ™‚é–“æª¢æŸ¥
                        pass
                    
                    # ç²å–æ–‡ç« æ‘˜è¦
                    print(f"æ­£åœ¨ç²å–æ‘˜è¦: {title[:40]}...")
                    summary = get_article_summary(link)
                    
                    # åˆ†é¡æ–°è
                    category = classify_news(title, summary)
                    
                    if category:
                        news_item = f"ğŸ“° {title}\nğŸ“ {summary}\nğŸ”— {link}\nğŸ“Œ ä¾†æºï¼š{source_name}"
                        news_by_category[category].append(news_item)
                        processed_titles.add(title_normalized)
                        print(f"âœ… å·²åˆ†é¡åˆ° [{category}]: {title[:40]}...")
                    else:
                        print(f"âš ï¸ æœªç¬¦åˆåˆ†é¡æ¢ä»¶: {title[:40]}...")
                    
                except Exception as e:
                    print(f"è™•ç†æ–°èé …ç›®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    continue
                    
        except Exception as e:
            print(f"è™•ç† RSS æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    
    return news_by_category

def push_line_message(message):
    """ç™¼é€ LINE è¨Šæ¯"""
    # æª¢æŸ¥ ACCESS_TOKEN æ˜¯å¦è¨­å®š
    if not ACCESS_TOKEN or ACCESS_TOKEN == 'ä½ çš„ LINE Channel Access Token':
        print("âš ï¸ ACCESS_TOKEN æœªæ­£ç¢ºè¨­å®šï¼Œè·³é LINE è¨Šæ¯ç™¼é€")
        print(f"ğŸ“ é è¦½è¨Šæ¯å…§å®¹:\n{message}")
        return
    
    # æª¢æŸ¥ USER_ID æ˜¯å¦è¨­å®š
    if not USER_ID or USER_ID == 'ä½ çš„ LINE User ID':
        print("âš ï¸ USER_ID æœªæ­£ç¢ºè¨­å®šï¼Œè·³é LINE è¨Šæ¯ç™¼é€")
        print(f"ğŸ“ é è¦½è¨Šæ¯å…§å®¹:\n{message}")
        return
    
    url = 'https://api.line.me/v2/bot/message/push'
    
    # ç¢ºä¿ ACCESS_TOKEN ç‚ºç´” ASCII å­—ç¬¦
    try:
        token = ACCESS_TOKEN.encode('ascii').decode('ascii')
    except UnicodeEncodeError:
        print("âš ï¸ ACCESS_TOKEN åŒ…å«é ASCII å­—ç¬¦ï¼Œè«‹æª¢æŸ¥ token è¨­å®š")
        return
    
    headers = {
        'Content-Type': 'application/json; charset=utf-8',
        'Authorization': f'Bearer {token}'
    }
    
    try:
        # å¦‚æœè¨Šæ¯å¤ªé•·ï¼Œåˆ†æ®µç™¼é€
        max_length = 4000
        if len(message) > max_length:
            parts = [message[i:i+max_length] for i in range(0, len(message), max_length)]
            for i, part in enumerate(parts):
                payload = {
                    "to": USER_ID,
                    "messages": [{
                        "type": "text",
                        "text": part
                    }]
                }
                response = requests.post(url, headers=headers, json=payload, timeout=30)
                print(f"ç™¼é€ç¬¬ {i+1} æ®µç‹€æ…‹: {response.status_code}")
                if response.status_code != 200:
                    print(f"ç™¼é€å¤±æ•—: {response.text}")
        else:
            payload = {
                "to": USER_ID,
                "messages": [{
                    "type": "text",
                    "text": message
                }]
            }
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            print(f"ç™¼é€ç‹€æ…‹: {response.status_code}")
            if response.status_code != 200:
                print(f"ç™¼é€å¤±æ•—: {response.text}")
            else:
                print("âœ… è¨Šæ¯ç™¼é€æˆåŠŸ")
                
    except Exception as e:
        print(f"âŒ ç™¼é€ LINE è¨Šæ¯æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    print("é–‹å§‹æŠ“å–é‡‘æ§å’Œä¿éšªç›¸é—œæ–°è...")
    print(f"ACCESS_TOKEN è¨­å®šç‹€æ…‹: {'âœ… å·²è¨­å®š' if ACCESS_TOKEN and ACCESS_TOKEN != 'ä½ çš„ LINE Channel Access Token' else 'âŒ æœªè¨­å®š'}")
    print(f"USER_ID è¨­å®šç‹€æ…‹: {'âœ… å·²è¨­å®š' if USER_ID and USER_ID != 'ä½ çš„ LINE User ID' else 'âŒ æœªè¨­å®š'}")
    
    news_by_category = fetch_news()
    
    # çµ±è¨ˆç¸½æ–°èæ•¸é‡
    total_news = sum(len(news_list) for news_list in news_by_category.values())
    print(f"\nğŸ“Š ç¸½å…±æ‰¾åˆ° {total_news} å‰‡ç¬¦åˆæ¢ä»¶çš„æ–°è")
    
    if total_news > 0:
        today = datetime.now(TW_TZ).strftime("%Y-%m-%d")
        
        for category, news_list in news_by_category.items():
            if news_list:
                message = f"ã€{today} {category}æ–°èæ•´ç†ã€‘\nå…± {len(news_list)} å‰‡æ–°è\n\n"
                message += "\n\n".join(news_list)
                push_line_message(message)
                print(f"âœ… å·²è™•ç† {category} æ–°è ({len(news_list)} å‰‡)")
        
        print("æ–°èè™•ç†å®Œæˆï¼")
    else:
        no_news_message = f"ã€{datetime.now(TW_TZ).strftime('%Y-%m-%d')} ä»Šæ—¥æ–°èã€‘\næš«ç„¡æ–°å…‰é‡‘æ§ã€å°æ–°é‡‘æ§æˆ–ä¿éšªç›¸é—œæ–°è"
        push_line_message(no_news_message)
        print("ä»Šæ—¥ç„¡ç¬¦åˆæ¢ä»¶çš„æ–°è")
