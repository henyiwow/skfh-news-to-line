import os
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import email.utils
from urllib.parse import quote
import requests
import re
from bs4 import BeautifulSoup

ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')
print("âœ… Access Token å‰ 10 ç¢¼ï¼š", ACCESS_TOKEN[:10] if ACCESS_TOKEN else "æœªè¨­å®š")

CATEGORY_KEYWORDS = {
    "æ–°å…‰é‡‘æ§": ["æ–°å…‰é‡‘", "æ–°å…‰äººå£½", "æ–°å£½", "å³æ±é€²"],
    "å°æ–°é‡‘æ§": ["å°æ–°é‡‘", "å°æ–°äººå£½", "å°æ–°å£½", "å³æ±äº®"],
    "é‡‘æ§": ["é‡‘æ§", "é‡‘èæ§è‚¡", "ä¸­ä¿¡é‡‘", "ç‰å±±é‡‘", "æ°¸è±é‡‘", "åœ‹æ³°é‡‘", "å¯Œé‚¦é‡‘", "å°ç£é‡‘"],
    "ä¿éšª": ["ä¿éšª", "å£½éšª", "å¥åº·éšª", "æ„å¤–éšª", "äººå£½"],
    "å…¶ä»–": []
}

EXCLUDED_KEYWORDS = ['ä¿éšªå¥—', 'é¿å­•å¥—', 'ä¿éšªå¥—ä½¿ç”¨', 'å¤ªé™½äººå£½', 'å¤§è¥¿éƒ¨äººå£½', 'ç¾åœ‹æµ·å²¸ä¿éšª']

TW_TZ = timezone(timedelta(hours=8))
now = datetime.now(TW_TZ)
today = now.date()

# âœ… æ¨™é¡Œæ­£è¦åŒ–
def normalize_title(title):
    title = re.sub(r'[ï½œ|â€§\-ï¼â€“â€”~ï½].*$', '', title)  # ç§»é™¤åª’é«”å¾Œç¶´
    title = re.sub(r'<[^>]+>', '', title)            # ç§»é™¤ HTML æ¨™ç±¤
    title = re.sub(r'[^\w\u4e00-\u9fff\s]', '', title)  # ç§»é™¤éæ–‡å­—ç¬¦è™Ÿ
    title = re.sub(r'\s+', ' ', title)               # å¤šé¤˜ç©ºç™½
    return title.strip().lower()

def get_article_summary(url, max_chars=100):
    """ç²å–æ–‡ç« æ‘˜è¦ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    try:
        # å…ˆæª¢æŸ¥æ˜¯å¦ç‚º Google News ç¶²å€
        if 'news.google.com' in url:
            print(f"    âš ï¸ Google News ç¶²å€ï¼Œå¯èƒ½ç„¡æ³•ç²å–æ‘˜è¦: {url[:60]}...")
            return "Google News é€£çµï¼Œè«‹é»æ“ŠæŸ¥çœ‹å®Œæ•´å…§å®¹"
        
        # æ›´å®Œæ•´çš„ headers
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
        }
        
        print(f"    ğŸ” å˜—è©¦ç²å–æ‘˜è¦: {url[:60]}...")
        response = requests.get(url, headers=headers, timeout=8, allow_redirects=True)
        
        # æª¢æŸ¥æœ€çµ‚ URL
        final_url = response.url
        print(f"    ğŸ“ æœ€çµ‚ URL: {final_url[:60]}...")
        
        # æª¢æŸ¥æ˜¯å¦æˆåŠŸç²å–å…§å®¹
        if response.status_code != 200:
            print(f"    âŒ HTTP ç‹€æ…‹ç¢¼: {response.status_code}")
            return f"ç„¡æ³•ç²å–æ‘˜è¦ï¼ˆç‹€æ…‹ç¢¼ï¼š{response.status_code}ï¼‰"
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # å…ˆå˜—è©¦å– meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            content_text = meta_desc.get('content').strip()
            print(f"    âœ… å¾ meta description ç²å–: {content_text[:50]}...")
        else:
            # å˜—è©¦ og:description
            og_desc = soup.find('meta', attrs={'property': 'og:description'})
            if og_desc and og_desc.get('content'):
                content_text = og_desc.get('content').strip()
                print(f"    âœ… å¾ og:description ç²å–: {content_text[:50]}...")
            else:
                print(f"    âš ï¸ ç„¡æ³•å¾ meta æ¨™ç±¤ç²å–æ‘˜è¦ï¼Œå˜—è©¦è§£æå…§å®¹...")
                
                # ç§»é™¤ä¸éœ€è¦çš„æ¨™ç±¤
                for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe', 'form', 'button']):
                    tag.decompose()
                
                # å˜—è©¦æ‰¾åˆ°æ–‡ç« å…§å®¹
                content_selectors = [
                    'article p', '.content p', '.article-content p', 
                    '.news-content p', '.post-content p', 'main p',
                    '.entry-content p', '.story-content p', '.article-body p'
                ]
                
                content_text = ""
                for selector in content_selectors:
                    try:
                        paragraphs = soup.select(selector)
                        if paragraphs:
                            valid_paragraphs = []
                            for p in paragraphs[:3]:
                                text = p.get_text().strip()
                                if len(text) > 30 and 'é»æ“Š' not in text and 'æ›´å¤š' not in text:
                                    valid_paragraphs.append(text)
                            
                            if valid_paragraphs:
                                content_text = " ".join(valid_paragraphs[:1])  # åªå–ç¬¬ä¸€æ®µ
                                print(f"    âœ… å¾å…§å®¹è§£æç²å–: {content_text[:50]}...")
                                break
                    except:
                        continue
                
                if not content_text:
                    print(f"    âŒ ç„¡æ³•è§£ææ–‡ç« å…§å®¹")
                    return "ç„¡æ³•ç²å–æ–‡ç« æ‘˜è¦"
        
        # æ¸…ç†å’Œæˆªå–æ–‡æœ¬
        if content_text:
            # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å’Œç‰¹æ®Šå­—ç¬¦
            content_text = re.sub(r'\s+', ' ', content_text)
            content_text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€Œã€ã€ã€ï¼ˆï¼‰ã€]', '', content_text)
            content_text = content_text.strip()
            
            # æˆªå–æŒ‡å®šå­—æ•¸
            if len(content_text) > max_chars:
                content_text = content_text[:max_chars] + "..."
            
            return content_text if content_text else "ç„¡æ³•ç²å–æ‘˜è¦"
        
        return "ç„¡æ³•ç²å–æ‘˜è¦"
        
    except requests.exceptions.Timeout:
        print(f"    â° è«‹æ±‚è¶…æ™‚")
        return "ç¶²ç«™å›æ‡‰è¶…æ™‚"
    except requests.exceptions.ConnectionError:
        print(f"    ğŸ”Œ é€£ç·šéŒ¯èª¤")
        return "ç¶²è·¯é€£ç·šéŒ¯èª¤"
    except Exception as e:
        print(f"    âŒ å…¶ä»–éŒ¯èª¤: {str(e)[:50]}")
        return f"ç²å–æ‘˜è¦å¤±æ•—"

def shorten_url(long_url):
    try:
        encoded_url = quote(long_url, safe='')
        api_url = f"http://tinyurl.com/api-create.php?url={encoded_url}"
        res = requests.get(api_url, timeout=5)
        if res.status_code == 200:
            return res.text.strip()
    except Exception as e:
        print("âš ï¸ çŸ­ç¶²å€å¤±æ•—ï¼š", e)
    return long_url

def classify_news(title):
    title = normalize_title(title)
    for category, keywords in CATEGORY_KEYWORDS.items():
        if any(kw.lower() in title for kw in keywords):
            return category
    return "å…¶ä»–"

def is_taiwan_news(source_name, link):
    taiwan_sources = [
        'å·¥å•†æ™‚å ±', 'ä¸­åœ‹æ™‚å ±', 'ç¶“æ¿Ÿæ—¥å ±', 'ä¸‰ç«‹æ–°èç¶²', 'è‡ªç”±æ™‚å ±', 'è¯åˆæ–°èç¶²',
        'é¡é€±åˆŠ', 'å°ç£é›…è™', 'é‰…äº¨ç¶²', 'ä¸­æ™‚æ–°èç¶²','Ettodayæ–°èé›²',
        'å¤©ä¸‹é›œèªŒ', 'å¥‡æ‘©æ–°è', 'ã€Šç¾ä»£ä¿éšªã€‹é›œèªŒ','é è¦‹é›œèªŒ'
    ]
    if any(taiwan_source in source_name for taiwan_source in taiwan_sources) and "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±" not in source_name:
        return True
    if '.tw' in link:
        return True
    return False

def is_similar_simple(title, known_titles):
    """ç°¡åŒ–ç‰ˆç›¸ä¼¼åº¦æª¢æ¸¬ï¼ˆä¸ä½¿ç”¨èªæ„æ¨¡å‹ï¼‰"""
    norm_title = normalize_title(title)
    
    for known_title in known_titles:
        # è¨ˆç®—å­—ç¬¦é‡ç–Šç‡
        title_set = set(norm_title)
        known_set = set(known_title)
        
        if len(title_set) == 0 or len(known_set) == 0:
            continue
            
        intersection = title_set.intersection(known_set)
        union = title_set.union(known_set)
        
        # å¦‚æœé‡ç–Šç‡è¶…é 80%ï¼Œèªç‚ºæ˜¯é‡è¤‡
        if len(union) > 0:
            similarity = len(intersection) / len(union)
            if similarity > 0.8:
                return True
    
    return False

def fetch_news():
    rss_urls = [
        "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å£½éšª+OR+é‡‘æ§+OR+äººå£½+OR+æ–°å£½+OR+å°æ–°å£½+OR+å³æ±é€²+OR+å³æ±äº®&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+æ–°å£½+OR+å³æ±é€²&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å°æ–°å£½+OR+å³æ±äº®&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=å£½éšª+OR+å¥åº·éšª+OR+æ„å¤–éšª+OR+äººå£½&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=é‡‘æ§+OR+é‡‘èæ§è‚¡&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    ]

    classified_news = {cat: [] for cat in CATEGORY_KEYWORDS}
    known_titles = []  # ä½¿ç”¨ç°¡å–®çš„å­—ç¬¦ä¸²åˆ—è¡¨

    for rss_url in rss_urls:
        res = requests.get(rss_url)
        print(f"âœ… ä¾†æº: {rss_url} å›æ‡‰ç‹€æ…‹ï¼š{res.status_code}")
        if res.status_code != 200:
            continue

        root = ET.fromstring(res.content)
        items = root.findall(".//item")
        print(f"âœ… å¾ {rss_url} æŠ“åˆ° {len(items)} ç­†æ–°è")

        for item in items:
            title_elem = item.find('title')
            link_elem = item.find('link')
            pubDate_elem = item.find('pubDate')
            if title_elem is None or link_elem is None or pubDate_elem is None:
                continue

            title = title_elem.text.strip()
            link = link_elem.text.strip()
            pubDate_str = pubDate_elem.text.strip()
            if not title or title.startswith("Google ãƒ‹ãƒ¥ãƒ¼ã‚¹"):
                continue

            source_elem = item.find('source')
            source_name = source_elem.text.strip() if source_elem is not None else "æœªæ¨™ç¤º"
            pub_datetime = email.utils.parsedate_to_datetime(pubDate_str).astimezone(TW_TZ)

            if now - pub_datetime > timedelta(hours=24):
                continue
            if any(bad_kw in title for bad_kw in EXCLUDED_KEYWORDS):
                continue
            if not is_taiwan_news(source_name, link):
                continue
            if is_similar_simple(title, known_titles):  # ä½¿ç”¨ç°¡åŒ–ç‰ˆç›¸ä¼¼åº¦æª¢æ¸¬
                continue

            # âœ… ç²å–æ–‡ç« æ‘˜è¦
            print(f"ğŸ“° æ­£åœ¨è™•ç†: {title[:40]}...")
            summary = get_article_summary(link)
            
            short_link = shorten_url(link)
            category = classify_news(title)
            
            # âœ… ä¿®æ”¹æ ¼å¼ï¼ŒåŠ å…¥æ‘˜è¦
            formatted = f"ğŸ“° {title}\nğŸ“ {summary}\nğŸ“Œ ä¾†æºï¼š{source_name}\nğŸ”— {short_link}"
            classified_news[category].append(formatted)

            # âœ… æ–°å¢æ¨™é¡Œåˆ°å·²çŸ¥åˆ—è¡¨ï¼ˆç”¨æ­£è¦åŒ–å¾Œæ¨™é¡Œï¼‰
            norm_title = normalize_title(title)
            known_titles.append(norm_title)

    return classified_news

def send_message_by_category(news_by_category):
    max_length = 4000
    
    # æ”¶é›†æ‰€æœ‰æœ‰æ–°èçš„åˆ†é¡
    categories_with_news = []
    categories_without_news = []
    
    for category, messages in news_by_category.items():
        if messages:
            categories_with_news.append((category, messages))
        else:
            categories_without_news.append(category)
    
    # æ§‹å»ºå®Œæ•´è¨Šæ¯
    full_message = ""
    
    # å…ˆåŠ å…¥æœ‰æ–°èçš„åˆ†é¡
    for category, messages in categories_with_news:
        category_section = f"ã€{today} æ¥­ä¼éƒ¨ ä»Šæ—¥ã€{category}ã€‘é‡é»æ–°èæ•´ç†ã€‘ å…±{len(messages)}å‰‡æ–°è\n\n"
        category_content = "\n\n".join(messages)
        category_section += category_content + "\n\n"
        
        # æª¢æŸ¥æ˜¯å¦æœƒè¶…éé•·åº¦é™åˆ¶
        if len(full_message + category_section) > max_length:
            # å¦‚æœæœƒè¶…éï¼Œå°±æˆªæ–·ä¸¦çµæŸ
            remaining_space = max_length - len(full_message) - 50  # ä¿ç•™ç©ºé–“çµ¦æˆªæ–·æç¤º
            if remaining_space > 100:  # å¦‚æœé‚„æœ‰è¶³å¤ ç©ºé–“
                truncated_section = category_section[:remaining_space] + "...\n\nğŸ“ è¨Šæ¯å·²æˆªæ–·ï¼Œæ›´å¤šæ–°èè«‹æŸ¥çœ‹å¾ŒçºŒé€šçŸ¥"
                full_message += truncated_section
            else:
                full_message += "ğŸ“ æ›´å¤šæ–°èå› å­—æ•¸é™åˆ¶å·²çœç•¥"
            break
        else:
            full_message += category_section
    
    # å¦‚æœé‚„æœ‰ç©ºé–“ï¼ŒåŠ å…¥ç„¡æ–°èçš„åˆ†é¡
    if categories_without_news and len(full_message) < max_length - 200:
        no_news_section = f"ã€{today} æ¥­ä¼éƒ¨ ä»Šæ—¥ç„¡ç›¸é—œæ–°èåˆ†é¡æ•´ç†ã€‘\n"
        no_news_content = "\n".join(f"ğŸ“‚ã€{cat}ã€‘ç„¡ç›¸é—œæ–°è" for cat in categories_without_news)
        no_news_section += no_news_content
        
        if len(full_message + no_news_section) <= max_length:
            full_message += no_news_section
    
    # ğŸ†• å…ˆé¡¯ç¤ºè¨Šæ¯å…§å®¹
    print("\n" + "="*60)
    print("ğŸ“± å®Œæ•´è¨Šæ¯å…§å®¹é è¦½ï¼š")
    print("="*60)
    print(full_message)
    print("="*60 + "\n")
    
    # ç™¼é€å–®ä¸€è¨Šæ¯
    if full_message.strip():
        broadcast_message(full_message.strip())
    else:
        # å¦‚æœæ²’æœ‰ä»»ä½•å…§å®¹ï¼Œç™¼é€ç°¡å–®è¨Šæ¯
        simple_message = f"ã€{today} æ¥­ä¼éƒ¨ ä»Šæ—¥æ–°èæ•´ç†ã€‘\næš«ç„¡ç›¸é—œæ–°è"
        print("\n" + "="*60)
        print("ğŸ“± ç°¡å–®è¨Šæ¯å…§å®¹ï¼š")
        print("="*60)
        print(simple_message)
        print("="*60 + "\n")
        broadcast_message(simple_message)

def broadcast_message(message):
    if not ACCESS_TOKEN or ACCESS_TOKEN == "æœªè¨­å®š":
        print("âš ï¸ ACCESS_TOKEN æœªè¨­å®šï¼Œåƒ…é¡¯ç¤ºè¨Šæ¯å…§å®¹ï¼š")
        print("=" * 50)
        print(message)
        print("=" * 50)
        return
    
    url = 'https://api.line.me/v2/bot/message/broadcast'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {ACCESS_TOKEN}'
    }

    data = {
        "messages": [{
            "type": "text",
            "text": message
        }]
    }

    print(f"ğŸ“¤ ç™¼é€è¨Šæ¯ç¸½é•·ï¼š{len(message)} å­—å…ƒ")
    res = requests.post(url, headers=headers, json=data)
    print(f"ğŸ“¤ LINE å›å‚³ç‹€æ…‹ç¢¼ï¼š{res.status_code}")
    print("ğŸ“¤ LINE å›å‚³å…§å®¹ï¼š", res.text)

if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹æŠ“å–é‡‘æ§å’Œä¿éšªç›¸é—œæ–°è...")
    news = fetch_news()
    if news:
        send_message_by_category(news)
    else:
        print("âš ï¸ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ–°èï¼Œä¸ç™¼é€ã€‚")
