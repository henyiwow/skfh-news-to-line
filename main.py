import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import email.utils
from datetime import datetime, timedelta, timezone
import re

# è¨­å®šä½ çš„ LINE Bot Token èˆ‡ User ID
ACCESS_TOKEN = 'ä½ çš„ LINE Channel Access Token'
USER_ID = 'ä½ çš„ LINE User ID'

# å°ç£æ™‚å€
TW_TZ = timezone(timedelta(hours=8))

# é—œéµå­—å®šç¾©
KEYWORDS = {
    "æ–°å…‰é‡‘æ§": ["æ–°å…‰é‡‘æ§", "æ–°å…‰äººå£½", "æ–°å£½", "å³æ±é€²"],
    "å°æ–°é‡‘æ§": ["å°æ–°é‡‘æ§", "å°æ–°äººå£½", "å°æ–°å£½", "å³æ±äº®"],
    "ä¿éšªç›¸é—œ": ["ä¿éšª", "å£½éšª", "å¥åº·éšª", "æ„å¤–éšª", "äººå£½ä¿éšª", "ç”¢éšª"]
}

# æ’é™¤é—œéµå­—
EXCLUDED_KEYWORDS = ['ä¿éšªå¥—', 'é¿å­•å¥—', 'å¤ªé™½äººå£½', 'å¤§è¥¿éƒ¨äººå£½']

def classify_news(title, content):
    """æ–°èåˆ†é¡å‡½æ•¸"""
    text = (title + " " + content).lower()
    
    for category, keywords in KEYWORDS.items():
        if any(keyword.lower() in text for keyword in keywords):
            return category
    return None

def get_article_summary(url, max_chars=100):
    """ç²å–æ–‡ç« æ‘˜è¦"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç§»é™¤ä¸éœ€è¦çš„æ¨™ç±¤
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            tag.decompose()
        
        # å°‹æ‰¾æ–‡ç« å…§å®¹çš„å¸¸è¦‹æ¨™ç±¤
        content_selectors = [
            'article p', '.content p', '.article-content p', 
            '.news-content p', '.post-content p', 'main p',
            '.entry-content p', '.story-content p'
        ]
        
        content_text = ""
        for selector in content_selectors:
            paragraphs = soup.select(selector)
            if paragraphs:
                content_text = " ".join([p.get_text().strip() for p in paragraphs[:3]])
                break
        
        # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šé¸æ“‡å™¨ï¼Œå˜—è©¦æ‰€æœ‰ p æ¨™ç±¤
        if not content_text:
            paragraphs = soup.find_all('p')
            content_text = " ".join([p.get_text().strip() for p in paragraphs[:3]])
        
        # æ¸…ç†æ–‡æœ¬
        content_text = re.sub(r'\s+', ' ', content_text)
        content_text = content_text.strip()
        
        # æˆªå–æŒ‡å®šå­—æ•¸
        if len(content_text) > max_chars:
            content_text = content_text[:max_chars] + "..."
        
        return content_text if content_text else "ç„¡æ³•ç²å–æ‘˜è¦"
        
    except Exception as e:
        print(f"ç²å–æ‘˜è¦å¤±æ•—: {e}")
        return "ç„¡æ³•ç²å–æ‘˜è¦"

def fetch_news():
    """å¾ RSS ç²å–æ–°è"""
    rss_urls = [
        "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+æ–°å£½+OR+å³æ±é€²&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å°æ–°å£½+OR+å³æ±äº®&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=å£½éšª+OR+ä¿éšª+OR+äººå£½ä¿éšª&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    ]
    
    news_by_category = {category: [] for category in KEYWORDS.keys()}
    processed_titles = set()
    
    for rss_url in rss_urls:
        try:
            print(f"æ­£åœ¨æŠ“å–: {rss_url}")
            response = requests.get(rss_url, timeout=10)
            
            if response.status_code != 200:
                continue
                
            root = ET.fromstring(response.content)
            items = root.findall(".//item")
            print(f"æ‰¾åˆ° {len(items)} å‰‡æ–°è")
            
            for item in items:
                try:
                    title_elem = item.find('title')
                    link_elem = item.find('link')
                    pubDate_elem = item.find('pubDate')
                    
                    if not all([title_elem, link_elem, pubDate_elem]):
                        continue
                    
                    title = title_elem.text.strip()
                    link = link_elem.text.strip()
                    pubDate_str = pubDate_elem.text.strip()
                    
                    # è·³éç„¡æ•ˆæ¨™é¡Œ
                    if not title or title.startswith("Google"):
                        continue
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚ºé‡è¤‡æ–°è
                    if title in processed_titles:
                        continue
                    
                    # æª¢æŸ¥æ’é™¤é—œéµå­—
                    if any(excluded in title for excluded in EXCLUDED_KEYWORDS):
                        continue
                    
                    # æª¢æŸ¥ç™¼å¸ƒæ™‚é–“ï¼ˆ24å°æ™‚å…§ï¼‰
                    try:
                        pub_datetime = email.utils.parsedate_to_datetime(pubDate_str).astimezone(TW_TZ)
                        now = datetime.now(TW_TZ)
                        if now - pub_datetime > timedelta(hours=24):
                            continue
                    except:
                        continue
                    
                    # ç²å–æ–‡ç« æ‘˜è¦
                    print(f"æ­£åœ¨ç²å–æ‘˜è¦: {title[:30]}...")
                    summary = get_article_summary(link)
                    
                    # åˆ†é¡æ–°è
                    category = classify_news(title, summary)
                    
                    if category:
                        news_item = f"ğŸ“° {title}\nğŸ“ {summary}\nğŸ”— {link}"
                        news_by_category[category].append(news_item)
                        processed_titles.add(title)
                        print(f"âœ… å·²åˆ†é¡åˆ° [{category}]: {title[:30]}...")
                    
                except Exception as e:
                    print(f"è™•ç†æ–°èé …ç›®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    continue
                    
        except Exception as e:
            print(f"è™•ç† RSS æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    
    return news_by_category

def push_line_message(message):
    """ç™¼é€ LINE è¨Šæ¯"""
    url = 'https://api.line.me/v2/bot/message/push'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {ACCESS_TOKEN}'
    }
    
    # å¦‚æœè¨Šæ¯å¤ªé•·ï¼Œåˆ†æ®µç™¼é€
    max_length = 4000
    if len(message) > max_length:
        parts = [message[i:i+max_length] for i in range(0, len(message), max_length)]
        for part in parts:
            payload = {
                "to": USER_ID,
                "messages": [{
                    "type": "text",
                    "text": part
                }]
            }
            response = requests.post(url, headers=headers, json=payload)
            print(f"ç™¼é€ç‹€æ…‹: {response.status_code}")
    else:
        payload = {
            "to": USER_ID,
            "messages": [{
                "type": "text",
                "text": message
            }]
        }
        response = requests.post(url, headers=headers, json=payload)
        print(f"ç™¼é€ç‹€æ…‹: {response.status_code}, å›æ‡‰: {response.text}")

if __name__ == "__main__":
    print("é–‹å§‹æŠ“å–é‡‘æ§å’Œä¿éšªç›¸é—œæ–°è...")
    news_by_category = fetch_news()
    
    if any(news_by_category.values()):
        today = datetime.now(TW_TZ).strftime("%Y-%m-%d")
        
        for category, news_list in news_by_category.items():
            if news_list:
                message = f"ã€{today} {category}æ–°èæ•´ç†ã€‘\nå…± {len(news_list)} å‰‡æ–°è\n\n"
                message += "\n\n".join(news_list)
                push_line_message(message)
                print(f"âœ… å·²ç™¼é€ {category} æ–°è ({len(news_list)} å‰‡)")
        
        print("æ–°èç™¼é€å®Œæˆï¼")
    else:
        print("ä»Šæ—¥ç„¡ç¬¦åˆæ¢ä»¶çš„æ–°è")
        push_line_message("ã€ä»Šæ—¥æ–°èã€‘\næš«ç„¡æ–°å…‰é‡‘æ§ã€å°æ–°é‡‘æ§æˆ–ä¿éšªç›¸é—œæ–°è")
